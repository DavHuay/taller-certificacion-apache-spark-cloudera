
Lenovo@DESKTOP-2RI54IU MINGW64 /e/spark
$ vagrant init
A `Vagrantfile` has been placed in this directory. You are now
ready to `vagrant up` your first virtual environment! Please read
the comments in the Vagrantfile as well as documentation on
`vagrantup.com` for more information on using Vagrant.

Lenovo@DESKTOP-2RI54IU MINGW64 /e/spark
$ vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Importing base box 'itversity/centos7spark'...
==> default: Matching MAC address for NAT networking...
==> default: Checking if box 'itversity/centos7spark' version '1.0' is up to date.
==> default: Setting the name of the VM: spark_default_1658519480452_28316
==> default: Clearing any previously set network interfaces...
==> default: Preparing network interfaces based on configuration...
    default: Adapter 1: nat
==> default: Forwarding ports...
    default: 8888 (guest) => 8888 (host) (adapter 1)
    default: 4040 (guest) => 4040 (host) (adapter 1)
    default: 22 (guest) => 2222 (host) (adapter 1)
==> default: Running 'pre-boot' VM customizations...
==> default: Booting VM...
==> default: Waiting for machine to boot. This may take a few minutes...
    default: SSH address: 127.0.0.1:2222
    default: SSH username: vagrant
    default: SSH auth method: private key
==> default: Machine booted and ready!
==> default: Checking for guest additions in VM...
    default: The guest additions on this VM do not match the installed version of
    default: VirtualBox! In most cases this is fine, but in rare cases it can
    default: prevent things such as shared folders from working properly. If you s
    default: shared folder errors, please make sure the guest additions within the
    default: virtual machine match the version of VirtualBox you have installed on
    default: your host and reload your VM.
    default:
    default: Guest Additions Version: 6.0.14
    default: VirtualBox Version: 6.1
==> default: Mounting shared folders...
    default: /vagrant => E:/spark

Lenovo@DESKTOP-2RI54IU MINGW64 /e/spark
$ vagrant ssh
Last login: Fri Apr  3 23:25:11 2020 from 10.0.2.2
[vagrant@localhost ~]$  start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [localhost.localdomain]
[vagrant@localhost ~]$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
[vagrant@localhost ~]$ hdfs dfs –put /data/retail_db /public
–put: Unknown command
Usage: hadoop fs [generic options]
        [-appendToFile <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-checksum <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <ds
        [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]
        [-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]
        [-createSnapshot <snapshotDir> [<snapshotName>]]
        [-deleteSnapshot <snapshotDir> <snapshotName>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] [-v] [-x] <path> ...]
        [-expunge [-immediate]]
        [-find <path> ... <expression> ...]
        [-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-getfacl [-R] <path>]
        [-getfattr [-R] {-n name | -d} [-e en] <path>]
        [-getmerge [-nl] [-skip-empty-file] <src> <localdst>]
        [-head <file>]
        [-help [cmd ...]]
        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]
        [-renameSnapshot <snapshotDir> <oldName> <newName>]
        [-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path
        [-setfattr {-n name [-v value] | -x name} <path>]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] [-s <sleep interval>] <file>]
        [-test -[defswrz] <path>]
        [-text [-ignoreCrc] <src> ...]
        [-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]
        [-touchz <path> ...]
        [-truncate [-w] <length> <path> ...]
        [-usage [cmd ...]]

Generic options supported are:
-conf <configuration file>        specify an application configuration file
-D <property=value>               define a value for a given property
-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overridproperty from configurations.
-jt <local|resourcemanager:port>  specify a ResourceManager
-files <file1,...>                specify a comma-separated list of files to be coduce cluster
-libjars <jar1,...>               specify a comma-separated list of jar files to bclasspath
-archives <archive1,...>          specify a comma-separated list of archives to be compute machines

The general command line syntax is:
command [genericOptions] [commandOptions]

[vagrant@localhost ~]$ hdfs dfs -put /data/retail_db /public
2022-07-22 19:55:02,620 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:03,438 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:03,896 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:04,393 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:04,870 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:05,342 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:05,791 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:06,248 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:06,711 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:07,148 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:07,186 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:07,647 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:08,132 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:08,608 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:08,658 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:09,159 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:09,631 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:09,691 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:09,738 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:10,201 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:10,252 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:10,309 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:10,780 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:11,239 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:11,282 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:11,741 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:11,796 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:12,292 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:12,332 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:12,368 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:12,402 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:12,858 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:12,886 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:13,380 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:13,422 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:13,885 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:13,916 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:14,373 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:14,415 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:14,462 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:14,933 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:14,993 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:15,523 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
2022-07-22 19:55:15,592 INFO sasl.SaslDataTransferClient: SASL encryption trust chted = false, remoteHostTrusted = false
[vagrant@localhost ~]$ hdfs dfs -ls /public/retail_db
Found 8 items
drwxr-xr-x   - vagrant supergroup          0 2022-07-22 19:55 /public/retail_db/.g
-rw-r--r--   1 vagrant supergroup        690 2022-07-22 19:55 /public/retail_db/RE
drwxr-xr-x   - vagrant supergroup          0 2022-07-22 19:55 /public/retail_db/ca
drwxr-xr-x   - vagrant supergroup          0 2022-07-22 19:55 /public/retail_db/cu
drwxr-xr-x   - vagrant supergroup          0 2022-07-22 19:55 /public/retail_db/de
drwxr-xr-x   - vagrant supergroup          0 2022-07-22 19:55 /public/retail_db/or
drwxr-xr-x   - vagrant supergroup          0 2022-07-22 19:55 /public/retail_db/or
drwxr-xr-x   - vagrant supergroup          0 2022-07-22 19:55 /public/retail_db/pr
[vagrant@localhost ~]$ import org.apache.spark.sql.types._
 StructField("order_id", IntegerType, true),
 StructField("order_date", DateType, true),
 StructField("order_customer_id", IntegerType, true),
-bash: import: command not found
 StructField("order_status", StringType, true))
[vagrant@localhost ~]$ val customSchema = StructType(Array(
-bash: syntax error near unexpected token `('
[vagrant@localhost ~]$  StructField("order_id", IntegerType, true),
-bash: syntax error near unexpected token `"order_id",'
[vagrant@localhost ~]$  StructField("order_date", DateType, true),
-bash: syntax error near unexpected token `"order_date",'
[vagrant@localhost ~]$  StructField("order_customer_id", IntegerType, true),
-bash: syntax error near unexpected token `"order_customer_id",'
[vagrant@localhost ~]$  StructField("order_status", StringType, true))
-bash: syntax error near unexpected token `"order_status",'
[vagrant@localhost ~]$ spark-shell --packages org.apache.spark:spark-avro_2.11:2.4
Ivy Default Cache set to: /home/vagrant/.ivy2/cache
The jars for the packages stored in: /home/vagrant/.ivy2/jars
:: loading settings :: url = jar:file:/opt/spark-2.4.5-bin-hadoop2.7/jars/ivy-2.4.ivy/core/settings/ivysettings.xml
org.apache.spark#spark-avro_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-7af09c48-11b5-42ec;1.0
        confs: [default]
        found org.apache.spark#spark-avro_2.11;2.4.4 in central
        found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/4.4.jar ...
        [SUCCESSFUL ] org.apache.spark#spark-avro_2.11;2.4.4!spark-avro_2.11.jar (
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/un
        [SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (138ms)
:: resolution report :: resolve 6662ms :: artifacts dl 350ms
        :: modules in use:
        org.apache.spark#spark-avro_2.11;2.4.4 from central in [default]
        org.spark-project.spark#unused;1.0.0 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
        ---------------------------------------------------------------------

:: problems summary ::
:::: ERRORS
        SERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/18/apache-18.jar

        SERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/park-parent_2.11/2.4.4/spark-parent_2.11-2.4.4.jar

        SERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/park-avro_2.11/2.4.4/spark-avro_2.11-2.4.4-javadoc.jar

        SERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/ss-parent/9/oss-parent-9.jar


:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
:: retrieving :: org.apache.spark#spark-submit-parent-7af09c48-11b5-4264-8dc1-f026
        confs: [default]
        2 artifacts copied, 0 already retrieved (185kB/16ms)
2022-07-22 20:07:36,454 WARN util.Utils: Your hostname, localhost.localdomain reso address: 127.0.0.1; using 10.0.2.15 instead (on interface eth0)
2022-07-22 20:07:36,455 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to
2022-07-22 20:07:36,912 WARN util.NativeCodeLoader: Unable to load native-hadoop latform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(
Spark context Web UI available at http://10.0.2.15:4040
Spark context available as 'sc' (master = local[*], app id = local-1658520469427).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.5
      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

scala> val customSchema = StructType(Array(
     |  StructField("order_id", IntegerType, true),
     |  StructField("order_date", DateType, true),
     |  StructField("order_customer_id", IntegerType, true),
     |  StructField("order_status", StringType, true))
     | )
customSchema: org.apache.spark.sql.types.StructType = StructType(StructField(orderue), StructField(order_date,DateType,true), StructField(order_customer_id,IntegerTield(order_status,StringType,true))

scala> val orders = spark.read.format("csv").option("inferSchema", "true").schema(("/public/retail_db/orders")
orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: date ... 2 mo

scala> orders.write.format("avro").save("/user/vagrant/lab1/pregunta1/resultado")

scala> val validate = spark.read.format("avro").load("/user/vagrant/lab1/pregunta1
validate: org.apache.spark.sql.DataFrame = [order_id: int, order_date: date ... 2

scala> validate.printSchema()
root
 |-- order_id: integer (nullable = true)
 |-- order_date: date (nullable = true)
 |-- order_customer_id: integer (nullable = true)
 |-- order_status: string (nullable = true)


scala> validate.count()
res2: Long = 68883

scala> validate.show()
+--------+----------+-----------------+---------------+
|order_id|order_date|order_customer_id|   order_status|
+--------+----------+-----------------+---------------+
|       1|2013-07-25|            11599|         CLOSED|
|       2|2013-07-25|              256|PENDING_PAYMENT|
|       3|2013-07-25|            12111|       COMPLETE|
|       4|2013-07-25|             8827|         CLOSED|
|       5|2013-07-25|            11318|       COMPLETE|
|       6|2013-07-25|             7130|       COMPLETE|
|       7|2013-07-25|             4530|       COMPLETE|
|       8|2013-07-25|             2911|     PROCESSING|
|       9|2013-07-25|             5657|PENDING_PAYMENT|
|      10|2013-07-25|             5648|PENDING_PAYMENT|
|      11|2013-07-25|              918| PAYMENT_REVIEW|
|      12|2013-07-25|             1837|         CLOSED|
|      13|2013-07-25|             9149|PENDING_PAYMENT|
|      14|2013-07-25|             9842|     PROCESSING|
|      15|2013-07-25|             2568|       COMPLETE|
|      16|2013-07-25|             7276|PENDING_PAYMENT|
|      17|2013-07-25|             2667|       COMPLETE|
|      18|2013-07-25|             1205|         CLOSED|
|      19|2013-07-25|             9488|PENDING_PAYMENT|
|      20|2013-07-25|             9198|     PROCESSING|
+--------+----------+-----------------+---------------+
only showing top 20 rows


scala> val customerSchema = StructType(Array(
     |  StructField("customer_id", IntegerType, true),
     |  StructField("customer_fname", StringType, true),
     |  StructField("customer_lname", StringType, true),
     |  StructField("customer_email", StringType, true),
     |  StructField("customer_password", StringType, true),
     |  StructField("customer_street", StringType, true),
     |  StructField("customer_state", StringType, true),
     |  StructField("customer_city", StringType, true),
     |  StructField("customer_zipcode", StringType, true)
     |  )
     | )
customerSchema: org.apache.spark.sql.types.StructType = StructType(StructField(cuspe,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StructField(customer_email,StringType,true), StructField(customer_password,StringTield(customer_street,StringType,true), StructField(customer_state,StringType,true)omer_city,StringType,true), StructField(customer_zipcode,StringType,true))

scala> val customer = spark.read.format("csv").option("inferSchema", "true").schemload("/public/retail_db/customers")
customer: org.apache.spark.sql.DataFrame = [customer_id: int, customer_fname: strids]

scala> customer.write.format("parquet").save("/user/vagrant/lab1/pregunta2/resulta

scala> val orders_avro = spark.read.format("avro").load("/user/vagrant/lab1/pregun
orders_avro: org.apache.spark.sql.DataFrame = [order_id: int, order_date: date ...

scala> orders_avro.show()
+--------+----------+-----------------+---------------+
|order_id|order_date|order_customer_id|   order_status|
+--------+----------+-----------------+---------------+
|       1|2013-07-25|            11599|         CLOSED|
|       2|2013-07-25|              256|PENDING_PAYMENT|
|       3|2013-07-25|            12111|       COMPLETE|
|       4|2013-07-25|             8827|         CLOSED|
|       5|2013-07-25|            11318|       COMPLETE|
|       6|2013-07-25|             7130|       COMPLETE|
|       7|2013-07-25|             4530|       COMPLETE|
|       8|2013-07-25|             2911|     PROCESSING|
|       9|2013-07-25|             5657|PENDING_PAYMENT|
|      10|2013-07-25|             5648|PENDING_PAYMENT|
|      11|2013-07-25|              918| PAYMENT_REVIEW|
|      12|2013-07-25|             1837|         CLOSED|
|      13|2013-07-25|             9149|PENDING_PAYMENT|
|      14|2013-07-25|             9842|     PROCESSING|
|      15|2013-07-25|             2568|       COMPLETE|
|      16|2013-07-25|             7276|PENDING_PAYMENT|
|      17|2013-07-25|             2667|       COMPLETE|
|      18|2013-07-25|             1205|         CLOSED|
|      19|2013-07-25|             9488|PENDING_PAYMENT|
|      20|2013-07-25|             9198|     PROCESSING|
+--------+----------+-----------------+---------------+
only showing top 20 rows


scala> orders_avro.select("order_id","order_status").write.option("compression", "ser/vagrant/lab1/pregunta3/resultado2")

scala> val customer = spark.read.format("parquet").load("/user/vagrant/lab1/pregun
customer: org.apache.spark.sql.DataFrame = [customer_id: int, customer_fname: strids]

scala> customer.createOrReplaceTempView("customer")

scala> val orders_avro = spark.read.format("avro").load("/user/vagrant/lab1/pregun
orders_avro: org.apache.spark.sql.DataFrame = [order_id: int, order_date: date ...

scala> orders_avro.createOrReplaceTempView("orders_avro")

scala> val result = spark.sql("select customer_id, customer_fname, customer_lname from customer a inner join orders_avro b on a.customer_id = order_customer_id groucustomer_lname, customer_fname having count(*) > 5 ")
2022-07-22 20:11:47,953 WARN DataNucleus.General: Plugin (Bundle) "org.datanucleustered. Ensure you dont have multiple JAR versions of the same plugin in the classp:/opt/spark/jars/datanucleus-core-3.2.10.jar" is already registered, and you are tan identical plugin located at URL "file:/opt/spark-2.4.5-bin-hadoop2.7/jars/datan.jar."
2022-07-22 20:11:47,959 WARN DataNucleus.General: Plugin (Bundle) "org.datanucleusdy registered. Ensure you dont have multiple JAR versions of the same plugin in thRL "file:/opt/spark/jars/datanucleus-api-jdo-3.2.6.jar" is already registered, and register an identical plugin located at URL "file:/opt/spark-2.4.5-bin-hadoop2.7/pi-jdo-3.2.6.jar."
2022-07-22 20:11:47,961 WARN DataNucleus.General: Plugin (Bundle) "org.datanucleuslready registered. Ensure you dont have multiple JAR versions of the same plugin ihe URL "file:/opt/spark/jars/datanucleus-rdbms-3.2.9.jar" is already registered, ato register an identical plugin located at URL "file:/opt/spark-2.4.5-bin-hadoop2.-rdbms-3.2.9.jar."
2022-07-22 20:11:53,882 WARN metastore.ObjectStore: Version information not found .metastore.schema.verification is not enabled so recording the schema version 1.2.
2022-07-22 20:11:54,046 WARN metastore.ObjectStore: Failed to get database defaultObjectException
2022-07-22 20:11:54,884 WARN metastore.ObjectStore: Failed to get database global_SuchObjectException
result: org.apache.spark.sql.DataFrame = [customer_id: int, customer_fname: string]

scala> result.sort(result.cant.desc()).write.format("json").save("/user/vagrant/lan/resultado")
<console>:29: error: value cant is not a member of org.apache.spark.sql.DataFrame
       result.sort(result.cant.desc()).write.format("json").save("/user/vagrant/lan/resultado")
                          ^

scala> val ProductSchema = StructType(Array(
     |  StructField("product_id", IntegerType, true),
     |  StructField("product_category_id", IntegerType, true),
     |  StructField("product_name", StringType, true),
     |  StructField("product_description", StringType, true),
     |  StructField("product_price", FloatType, true),
     |  StructField("product_image", StringType, true)
     |  )
     | )
ProductSchema: org.apache.spark.sql.types.StructType = StructType(StructField(prod,true), StructField(product_category_id,IntegerType,true), StructField(product_nam, StructField(product_description,StringType,true), StructField(product_price,FloatField(product_image,StringType,true))

scala> val product = spark.read.format("csv").option("inferSchema", "true").schemaad("/public/retail_db/products")
product: org.apache.spark.sql.DataFrame = [product_id: int, product_category_id: ids]

scala> product.createOrReplaceTempView("product")

scala> val result =spark.sql("select product_id, max(product_price) as max_price fby product_id")
result: org.apache.spark.sql.DataFrame = [product_id: int, max_price: float]

scala> result.createOrReplaceTempView("result")

scala> val result2 =spark.sql("select concat(product_id, '|', max_price) as data f
result2: org.apache.spark.sql.DataFrame = [data: string]

scala> result2.repartition(1).write.option("compression","gzip").format("text").salab1/pregunta5/resultado")

scala> val customer = spark.read.format("parquet").load("/user/vagrant/lab1/pregun
customer: org.apache.spark.sql.DataFrame = [customer_id: int, customer_fname: strids]

scala> customer.createOrReplaceTempView("customer")

scala> val result = spark.sql("select customer_id, concat(substring(customer_fnamer_lname) as name , customer_street from customer")
result: org.apache.spark.sql.DataFrame = [customer_id: int, name: string ... 1 mor

scala> result.map(x => x.mkString("\t")).write.option("compression","bzip2").formaser/vagrant/lab1/pregunta6/resultado")

scala> val product = spark.read.format("csv").option("inferSchema", "true").schemaad("/public/retail_db/products")
product: org.apache.spark.sql.DataFrame = [product_id: int, product_category_id: ids]

scala> product.write.format("hive").saveAsTable("product")
2022-07-22 20:15:35,609 WARN metastore.HiveMetaStore: Location: file:/home/vagrantroduct specified for non-external table:product
2022-07-22 20:15:37,150 ERROR hdfs.KeyProviderCache: Could not find uri with key [.provider.uri] to create a keyProvider !!

scala> val orders = spark.read.format("csv").option("inferSchema", "true").schema(customSchema).load("/public/retail_db/orders")
orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: date ... 2 more fields]

scala> orders.write.format("hive").saveAsTable("orders")
2022-07-22 20:24:22,853 WARN metastore.HiveMetaStore: Location: file:/home/vagrant/spark-warehouse/orders specified for non-external table:orders

scala> val result = spark.sql("select count(*) as count,date_format(order_date,'YYYYMM') as month from orders group by date_format(order_date, 'YYYYMM')")
result: org.apache.spark.sql.DataFrame = [count: bigint, month: string]

scala> result.write.option("compression","uncompressed").format("parquet").save("/user/vagrant/lab1/pregunta8/resultado")

scala> val itemsSchema = StructType(Array(
     |  StructField("order_item_id", IntegerType, true),
     |  StructField("order_item_order_id", IntegerType, true),
     |  StructField("order_item_product_id", IntegerType, true),
     |  StructField("order_item_quantity", IntegerType, true),
     |  StructField("order_item_subtotal", FloatType, true),
     |  StructField("order_item_productprice", FloatType, true)
     |  )
     | )
itemsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(order_item_id,IntegerType,true), StructField(order_item_order_id,IntegerType,true), StructField(order_item_product_id,IntegerType,true), StructField(order_item_quantity,IntegerType,true), StructField(order_item_subtotal,FloatType,true), StructField(order_item_productprice,FloatType,true))

scala> val order_items= spark.read.format("csv").option("inferSchema", "true").schema(itemsSchema).load("/public/retail_db/order_items")
order_items: org.apache.spark.sql.DataFrame = [order_item_id: int, order_item_order_id: int ... 4 more fields]

scala> order_items.write.format("orc").option("compression","uncompressed").save("/user/vagrant/lab1/pregunta9/resultado")

scala> val customer = spark.read.format("parquet").load("/user/vagrant/lab1/pregunta2/resultado")
customer: org.apache.spark.sql.DataFrame = [customer_id: int, customer_fname: string ... 7 more fields]

scala> customer.createOrReplaceTempView("customer")

scala> val order_items = spark.read.format("orc").load("/user/vagrant/lab1/pregunta9/resultado")
order_items: org.apache.spark.sql.DataFrame = [order_item_id: int, order_item_order_id: int ... 4 more fields]

scala> order_items.createOrReplaceTempView("order_items")

scala>

scala>

scala>

scala> val top_customer = spark.sql("select customer_id, customer_fname, count(*) as cant, sum(order_item_subtotal) as total from customer a inner join orders b on a.customer_id = b.order_customer_id inner join order_items c on c.order_item_order_id = b.order_id where customer_city like 'M%' group by customer_id, customer_fname")
top_customer: org.apache.spark.sql.DataFrame = [customer_id: int, customer_fname: string ... 2 more fields]

scala> top_customer.write.format("parquet").option("compression","gzip").save("/user/vagrant/lab1/pregunta10/resultado")

scala>
